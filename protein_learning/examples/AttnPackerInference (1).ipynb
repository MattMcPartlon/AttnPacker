{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matthewmcpartlon/VSCode/AttnPacker\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "code_root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "if code_root not in sys.path:\n",
    "    sys.path.append(code_root)\n",
    "print(code_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Union\n",
    "from torch import Tensor\n",
    "\n",
    "import protein_learning.models.model_abc.train as sc\n",
    "from protein_learning.models.utils.model_io import (\n",
    "    load_n_save_args,\n",
    "    print_args,\n",
    "    get_args_n_groups,\n",
    "    load_args_for_eval,\n",
    ")\n",
    "from protein_learning.models.fbb_design.train import Train as SCPTrain, _augment\n",
    "from protein_learning.common.data.datasets.utils import set_canonical_coords_n_masks\n",
    "from protein_learning.common.data.data_types.protein import Protein\n",
    "import protein_learning.common.protein_constants as pc\n",
    "from protein_learning.features.input_embedding import InputEmbedding\n",
    "from protein_learning.models.utils.dataset_augment_fns import impute_cb\n",
    "from protein_learning.common.data.data_types.model_input import ModelInput\n",
    "\n",
    "\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def default(x, y):\n",
    "    return x if exists(x) else y\n",
    "\n",
    "\n",
    "def _parse_args(arg_stream):\n",
    "    args = []\n",
    "    for x in arg_stream:  # noqa\n",
    "        line = x.strip()\n",
    "        if len(line.strip()) > 0 and not line.startswith(\"#\"):\n",
    "            arg = line.split(\" \")\n",
    "            for a in arg:\n",
    "                args.append(a)\n",
    "    return args\n",
    "\n",
    "\n",
    "def parse_args(arg_path=None, arg_list=None, arg_string=None):\n",
    "    if exists(arg_list):\n",
    "        return arg_list\n",
    "    elif exists(arg_path):\n",
    "        with open(arg_path, \"r\") as f:\n",
    "            return _parse_args(f)\n",
    "    elif exists(arg_string):\n",
    "        return _parse_args(arg_string.split(\"\\n\"))\n",
    "    else:\n",
    "        raise Exception(\"All inputs are None!\")\n",
    "        \n",
    "def make_predicted_protein(model_out, seq: Optional[Union[str,Tensor]] = None) -> Protein:\n",
    "    \"\"\"Constructs predicted protein\"\"\"\n",
    "    if torch.is_tensor(seq):\n",
    "        seq = \"\".join([pc.INDEX_TO_AA_ONE[x.item()] for x in seq.squeeze()])\n",
    "    coords = model_out.predicted_coords.squeeze(0)\n",
    "    pred_protein = model_out.decoy_protein.from_coords_n_seq(coords,seq)\n",
    "    pred_protein = set_canonical_coords_n_masks(pred_protein, overwrite=True)\n",
    "    return pred_protein\n",
    "        \n",
    "        \n",
    "INFERENCE_ARGS = \"\"\"\n",
    "# whether todesign sequence\n",
    "--mask_seq\n",
    "# Don't mask any residues in input sequence\n",
    "--no_mask_weight 1\n",
    "--inter_no_mask_weight 1\n",
    "#--no_predict_from_angles\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Inference:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_n_config_root: str,\n",
    "        use_design_model: bool = True,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        if use_design_model:\n",
    "            model_name = 'fbb_design_21_12_2022_16:07:51'\n",
    "        else:\n",
    "            model_name = 'fbb_design_21_12_2022_15:58:28'\n",
    "        self.model_name = model_name\n",
    "        self.use_design_model = use_design_model\n",
    "        self.verbose = verbose\n",
    "        self.trainer = SCPTrain(skip_init=True)\n",
    "        self.resource_root = model_n_config_root\n",
    "        self.load_inference_args()\n",
    "        self.trainer.pad_embeddings = self.args.mask_seq or self.args.mask_feats\n",
    "        self.trainer._setup()\n",
    "\n",
    "    def load_inference_args(self): \n",
    "        eval_parser = sc.get_default_parser_for_eval()\n",
    "        self.trainer.add_extra_cmd_line_options_for_eval(eval_parser)\n",
    "        # default inference args\n",
    "        arg_list = parse_args(arg_string=INFERENCE_ARGS)\n",
    "        eval_args, eval_groups = get_args_n_groups(eval_parser, arg_list)\n",
    "        self.trainer.eval_args = eval_args\n",
    "        self.trainer.eval_groups = eval_groups\n",
    "\n",
    "        train_parser = self.trainer.add_extra_cmd_line_options(sc.get_default_parser())\n",
    "        train_args, train_groups = get_args_n_groups(train_parser, [\"none\"])  # defaults only\n",
    "\n",
    "        global_override = sc.default_global_override_for_eval(eval_args)\n",
    "        global_override.update(self.trainer.global_override_eval)  # args that should always be overridden\n",
    "        rr = self.resource_root\n",
    "        config, args, arg_groups = load_args_for_eval(\n",
    "            global_config_path=os.path.join(rr,\"params\",f\"{self.model_name}.npy\"),\n",
    "            model_config_path=os.path.join(rr,\"params\",f\"{self.model_name}_fbb_design.npy\"),\n",
    "            model_override=self.trainer.model_override_eval,\n",
    "            global_override=global_override,\n",
    "            default_model_args=train_args,\n",
    "            default_model_arg_groups=train_groups,\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"global override\\n {global_override}\")\n",
    "            print(f\"model override\\n {self.trainer.model_override_eval}\")\n",
    "        self.config, self.args, self.arg_groups = config, args, arg_groups\n",
    "        self.trainer.config = config\n",
    "        self.trainer.args=args\n",
    "        self.trainer.arg_groups=arg_groups\n",
    "        \n",
    "        return config, args, arg_groups\n",
    "    \n",
    "    def _init_model(self):\n",
    "        # set up feature generator\n",
    "        feature_config = sc.get_input_feature_config(\n",
    "            self.arg_groups,\n",
    "            pad_embeddings=self.trainer.pad_embeddings,\n",
    "            extra_pair_feat_dim=self.trainer.extra_pair_feat_dim,\n",
    "            extra_res_feat_dim=self.trainer.extra_res_feat_dim,\n",
    "        )\n",
    "        self.feature_config = feature_config  # noqa\n",
    "        feat_gen = sc.get_feature_gen(\n",
    "            self.arg_groups, feature_config, apply_masks=self.trainer.apply_masks\n",
    "        )\n",
    "        self.feat_gen = feat_gen  # noqa\n",
    "        self.input_embedding = InputEmbedding(feature_config)\n",
    "        self.model = self.trainer.get_model(self.input_embedding)\n",
    "    \n",
    "    def get_model(self):\n",
    "        self._init_model()\n",
    "        model_path = os.path.join(self.resource_root,\"models\",f\"{self.model_name}.tar\")\n",
    "        checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
    "        self.model.load_state_dict(checkpoint['model'],strict=False)\n",
    "        return self.model\n",
    "    \n",
    "    def load_example(\n",
    "        self,\n",
    "        pdb_path,\n",
    "        fasta_path = None,\n",
    "        seq_mask = None\n",
    "    ):\n",
    "        if exists(seq_mask):\n",
    "            assert self.use_design_model\n",
    "            \n",
    "        protein = Protein.FromPDBAndSeq(\n",
    "            pdb_path = pdb_path,\n",
    "            seq=fasta_path,\n",
    "            atom_tys = pc.ALL_ATOMS,\n",
    "            missing_seq = fasta_path is None,\n",
    "            load_ss=False,\n",
    "        )\n",
    "        protein, _ = impute_cb(protein,protein)\n",
    "        extra = _augment(protein, protein)\n",
    "        protein = set_canonical_coords_n_masks(protein)\n",
    "        \n",
    "        return ModelInput(\n",
    "            decoy=protein,\n",
    "            native=protein,\n",
    "            input_features=self.feat_gen.generate_features(\n",
    "                protein,\n",
    "                extra=extra,\n",
    "                seq_mask=seq_mask,\n",
    "            ),\n",
    "            extra=extra,\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] : no value for key help, arg_group : optional arguments\n",
      "[INFO] adding embeddings for res_ty: [1]\n",
      "[INFO] adding embeddings for rel_pos: [1]\n",
      "[INFO] adding embeddings for bb_dihedral: [1]\n",
      "[INFO] adding embeddings for centrality: [0]\n",
      "[INFO] adding embeddings for rel_sep: []\n",
      "[INFO] adding embeddings for rel_dist: [0]\n",
      "[INFO] adding embeddings for tr_ori: [1]\n",
      "[INFO] adding embeddings for rel_chain: [1]\n",
      "------------------------------------------------------\n",
      "[INFO] Input feature embeddings\n",
      "\n",
      "----[INFO] SCALAR (dim = 115)\n",
      "\n",
      "--------res_ty\n",
      "------------one_hot : 23\n",
      "--------rel_pos\n",
      "------------one_hot : 11\n",
      "--------bb_dihedral\n",
      "------------one_hot : 75\n",
      "--------centrality\n",
      "------------rbf : 6\n",
      "\n",
      "----[INFO] PAIR (dim = 205)\n",
      "\n",
      "--------rel_sep\n",
      "--------rel_dist\n",
      "------------rbf : 64\n",
      "--------tr_ori\n",
      "------------one_hot : 75\n",
      "--------rel_chain\n",
      "------------one_hot : 6\n",
      "--------joint_pair_n_sep\n",
      "------------rel_sep : 60\n",
      "------------res_ty_a : 60\n",
      "------------res_ty_b : 60\n",
      "\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "RESOURCE_ROOT = \"/Users/matthewmcpartlon/Downloads/AttnPackerPTM\"\n",
    "runner = Inference(RESOURCE_ROOT, verbose=False, use_design_model=True)\n",
    "model = runner.get_model()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pdb_path = \"./pdbs/T1043.pdb\"\n",
    "design_mask = None\n",
    "mask_frac = 0\n",
    "if runner.use_design_model:\n",
    "    design_mask = None #torch.rand(79)>(1-mask_frac) #mask out 50% of residue identities at random\n",
    "example = runner.load_example(pdb_path = pdb_path, seq_mask = design_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewmcpartlon/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran inference in 3.334 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    model_out = model.forward(example.to(device), use_cycles = 1)\n",
    "    print(f\"Ran inference in {round(time.time() - start,3)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masked residue positions\n",
    "seq_mask = default(\n",
    "    example.input_features.seq_mask,torch.zeros(len(example.decoy)).bool()\n",
    ")\n",
    "\n",
    "# coords, pLDDT, and Sequence\n",
    "res_feats = model_out.scalar_output\n",
    "pred_coords = model_out.predicted_coords\n",
    "pred_seq_labels = model_out.decoy_protein.seq_encoding\n",
    "pred_seq_logits = None\n",
    "\n",
    "\n",
    "# Predicted pLDDT\n",
    "plddt_head = model.loss_fn.loss_fns[\"plddt\"]\n",
    "pred_plddt = plddt_head.get_expected_value(res_feats)\n",
    "pred_plddt = pred_plddt\n",
    "\n",
    "# Predicted Sequence\n",
    "if \"nsr\" in model.loss_fn and torch.any(seq_mask):\n",
    "    pred_seq_logits = model.loss_fn.loss_fns[\"nsr\"].get_predicted_logits(res_feats)\n",
    "    pred_seq_labels = torch.argmax(pred_seq_logits,dim=-1)\n",
    "\n",
    "out = pred_coords, pred_seq_labels, pred_seq_logits, pred_plddt\n",
    "fn = lambda x : x.detach().cpu().squeeze() if torch.is_tensor(x) else x\n",
    "pred_coords, pred_seq_labels, pred_seq_logits, pred_plddt = map(fn,out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write prediction to pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving pdb to ./attn_packer/examples/T1043_packed.pdb\n"
     ]
    }
   ],
   "source": [
    "pred_protein = make_predicted_protein(model_out, seq = pred_seq_labels)\n",
    "pdb_out_path = f\"./attn_packer/examples/{pred_protein.name}_packed.pdb\"\n",
    "print(f\"saving pdb to {pdb_out_path}\")\n",
    "pred_protein.to_pdb(pdb_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Post-Processing Procedure to Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fn: project_onto_rotamers] : Using device cpu\n",
      "[INFO] Beginning rotamer projection\n",
      "[INFO] Initial loss values\n",
      "   [RMSD loss] = 3.133\n",
      "   [Steric loss] = 315.402\n",
      "   [Angle Dev. loss] = 0.0\n",
      "\n",
      "beginning iter: 0, steric weight: 0.5\n",
      "beginning iter: 1, steric weight: 0.5\n",
      "[INFO] Final Loss Values\n",
      "   [RMSD loss] = 3.124\n",
      "   [Steric loss] = 262.285\n",
      "   [Angle Dev. loss] = 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from protein_learning.protein_utils.sidechains.project_sidechains import project_onto_rotamers\n",
    "\n",
    "pp_pdb_out_path = f\"./attn_packer/examples/{pred_protein.name}_packed_pp.pdb\"\n",
    "\n",
    "projected_coords, _ = project_onto_rotamers(\n",
    "    atom_coords = pred_protein.atom_coords.unsqueeze(0),\n",
    "    sequence = pred_protein.seq_encoding.unsqueeze(0),\n",
    "    atom_mask = pred_protein.atom_masks.unsqueeze(0),\n",
    "    steric_clash_weight=[0.5],\n",
    ")\n",
    "\n",
    "pred_protein.to_pdb(pp_pdb_out_path, coords=projected_coords.squeeze())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out packing with NGL Viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Post-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nglview as nv\n",
    "view = nv.show_file(pdb_out_path)\n",
    "view.add_cartoon(\"protein\",color_scheme='residue_index')\n",
    "view.add_ball_and_stick(\"protein\")\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Post-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nglview as nv\n",
    "view = nv.show_file(pp_pdb_out_path)\n",
    "view.add_cartoon(\"protein\",color_scheme='residue_index')\n",
    "view.add_ball_and_stick(\"protein\")\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Predicted pLDDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(torch.arange(pred_plddt.numel()),pred_plddt.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Predicted Sequences (If Doing Sequence Design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_protein = make_predicted_protein(model_out, seq = pred_seq_labels)\n",
    "x,y = model_out.decoy_protein.seq, pred_protein.seq\n",
    "\n",
    "print(\"Input Sequence :\\n\",x)\n",
    "print(\"Predicted Sequence :\\n\",y)\n",
    "aln=\"\".join([\"|\" if x[i]!=y[i] else \" \" for i in range(len(x))])\n",
    "masked=\"\".join([\"*\" if seq_mask[i] else \" \" for i in range(len(x))])\n",
    "print(\"Aligned:\")\n",
    "print(f\" {x}\\n {aln}\\n {y}\\n {masked}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^ angles are being predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k in model.__dict__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_from_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, par in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.args.predict_from_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
