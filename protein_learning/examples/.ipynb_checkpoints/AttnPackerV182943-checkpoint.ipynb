{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b9297a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matthewmcpartlon/VSCode/protein_learning_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "code_root = os.getcwd()\n",
    "if code_root not in sys.path:\n",
    "    sys.path.append(code_root)\n",
    "print(code_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8acbbe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewmcpartlon/miniconda3/envs/py38/lib/python3.8/site-packages/Bio/SubsMat/__init__.py:126: BiopythonDeprecationWarning: Bio.SubsMat has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.substitution_matrices as a replacement, and contact the Biopython developers if you still need the Bio.SubsMat module.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from protein_learning.models.inference_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "511de8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] adding embeddings for res_ty: [1]\n",
      "[INFO] adding embeddings for rel_pos: [1]\n",
      "[INFO] adding embeddings for bb_dihedral: [1]\n",
      "[INFO] adding embeddings for centrality: [0]\n",
      "[INFO] adding embeddings for rel_sep: []\n",
      "[INFO] adding embeddings for rel_dist: [0]\n",
      "[INFO] adding embeddings for tr_ori: [1]\n",
      "[INFO] adding embeddings for rel_chain: [1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthewmcpartlon/VSCode/protein_learning_v2/protein_learning/models/inference_utils.py\", line 185, in get_model\n",
      "    self.model.load_state_dict(checkpoint[\"model\"], strict=True)\n",
      "  File \"/Users/matthewmcpartlon/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1667, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for FBBDesigner:\n",
      "\tUnexpected key(s) in state_dict: \"dihedral_loss.chi_pi_periodic_mask\", \"dihedral_loss.chi_angle_mask\". \n",
      "\n"
     ]
    }
   ],
   "source": [
    "RESOURCE_ROOT = \"/Users/matthewmcpartlon/Downloads/fbb_design_ft_inference/models\"\n",
    "MODEL_NAME = \"fbb_design_21_12_2022_16:00:06\" #RX7-1\n",
    "#MODEL_NAME = \"fbb_design_21_12_2022_15:57:43\" # RX11\n",
    "#MODEL_NAME = \"fbb_design_21_12_2022_16:07:51\" #RX7-2\n",
    "runner = Inference(RESOURCE_ROOT, MODEL_NAME)\n",
    "model = runner.get_model()\n",
    "device = \"cpu\"#\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a7a01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "pdb_path = \"/Users/matthewmcpartlon/Downloads/casp_all/T0967.pdb\"\n",
    "example = runner.load_example(pdb_path = pdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19485e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthewmcpartlon/miniconda3/envs/py38/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran inference in 1.424 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    model_out = model.forward(example.to(device), use_cycles = 1)\n",
    "    print(f\"Ran inference in {round(time.time() - start,3)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cebab5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#masked residue positions\n",
    "seq_mask = default(\n",
    "    example.input_features.seq_mask,torch.zeros(len(example.decoy)).bool()\n",
    ")\n",
    "\n",
    "# coords, pLDDT, and Sequence\n",
    "res_feats = model_out.scalar_output\n",
    "pred_coords = model_out.predicted_coords\n",
    "pred_seq_labels = model_out.decoy_protein.seq_encoding\n",
    "pred_seq_logits = None\n",
    "\n",
    "\n",
    "# Predicted pLDDT\n",
    "plddt_head = model.loss_fn.loss_fns[\"plddt\"]\n",
    "pred_plddt = plddt_head.get_expected_value(res_feats)\n",
    "pred_plddt = pred_plddt\n",
    "\n",
    "# Predicted Sequence\n",
    "if \"nsr\" in model.loss_fn and torch.any(seq_mask):\n",
    "    pred_seq_logits = model.loss_fn.loss_fns[\"nsr\"].get_predicted_logits(res_feats)\n",
    "    pred_seq_labels = torch.argmax(pred_seq_logits,dim=-1)\n",
    "\n",
    "out = pred_coords, pred_seq_labels, pred_seq_logits, pred_plddt\n",
    "fn = lambda x : x.detach().cpu().squeeze() if torch.is_tensor(x) else x\n",
    "pred_coords, pred_seq_labels, pred_seq_logits, pred_plddt = map(fn,out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003b5a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving pdb to ./attn_packer/examples/T0967_packed.pdb\n"
     ]
    }
   ],
   "source": [
    "pred_protein = make_predicted_protein(model_out, seq = pred_seq_labels)\n",
    "pdb_out_path = f\"./attn_packer/examples/{pred_protein.name}_packed.pdb\"\n",
    "print(f\"saving pdb to {pdb_out_path}\")\n",
    "pred_protein.to_pdb(pdb_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eebb9850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fn: project_onto_rotamers] : Using device cpu\n",
      "[INFO] Beginning rotamer projection\n",
      "[INFO] Initial loss values\n",
      "   [RMSD loss] = 2.513\n",
      "   [Steric loss] = 113.738\n",
      "   [Angle Dev. loss] = 0.0\n",
      "\n",
      "beginning iter: 0, steric weight: 0.5\n",
      "beginning iter: 1, steric weight: 0.5\n",
      "[INFO] Final Loss Values\n",
      "   [RMSD loss] = 2.512\n",
      "   [Steric loss] = 91.511\n",
      "   [Angle Dev. loss] = 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from protein_learning.protein_utils.sidechains.project_sidechains import project_onto_rotamers\n",
    "\n",
    "pp_pdb_out_path = f\"./attn_packer/examples/{pred_protein.name}_packed_pp.pdb\"\n",
    "\n",
    "projected_coords, _ = project_onto_rotamers(\n",
    "    atom_coords = pred_protein.atom_coords.unsqueeze(0),\n",
    "    sequence = pred_protein.seq_encoding.unsqueeze(0),\n",
    "    atom_mask = pred_protein.atom_masks.unsqueeze(0),\n",
    "    steric_clash_weight=[0.5],\n",
    ")\n",
    "\n",
    "pred_protein.to_pdb(pp_pdb_out_path, coords=projected_coords.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73f88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
